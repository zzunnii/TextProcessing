{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-04T05:58:28.628498Z",
     "start_time": "2025-04-04T05:58:28.613497Z"
    }
   },
   "source": [
    "docs = []\n",
    "docs.append('To do is to be. To be is to do.')\n",
    "docs.append('To be or not to be, I am what I am.')\n",
    "docs.append('I think therefore I am. Do be do be do.')\n",
    "docs.append('Do do do, da da da, Let it be, let it be')\n",
    "print(docs)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To do is to be. To be is to do.', 'To be or not to be, I am what I am.', 'I think therefore I am. Do be do be do.', 'Do do do, da da da, Let it be, let it be']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T05:56:07.682196Z",
     "start_time": "2025-04-04T05:56:02.373936Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install nltk",
   "id": "cf368d6d40dd2533",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: click in c:\\users\\tjdwn\\anaconda3\\envs\\ocr\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\tjdwn\\anaconda3\\envs\\ocr\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\tjdwn\\anaconda3\\envs\\ocr\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\tjdwn\\anaconda3\\envs\\ocr\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.5/1.5 MB 11.4 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp39-cp39-win_amd64.whl (274 kB)\n",
      "Installing collected packages: regex, nltk\n",
      "Successfully installed nltk-3.9.1 regex-2024.11.6\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T05:59:40.045494Z",
     "start_time": "2025-04-04T05:59:39.545415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ],
   "id": "2ba319e01d614b7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tjdwn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\tjdwn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T05:59:40.894262Z",
     "start_time": "2025-04-04T05:59:40.845113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "vocab = {}          # python dictionary\n",
    "doc_tokens = []     # python list\n",
    "for doc in docs:\n",
    "    tokens = word_tokenize(doc.lower()) #문서 토큰화\n",
    "    for word in tokens:\n",
    "        if word not in vocab: #사전에 없는 단어일 경우 \n",
    "            vocab[word] = 0   #사전에 추가\n",
    "        vocab[word] += 1    \n",
    "    doc_tokens.append(tokens)\n",
    "print(vocab)\n",
    "print(doc_tokens)"
   ],
   "id": "ee8478f39543a1e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 6, 'do': 8, 'is': 2, 'be': 8, '.': 5, 'or': 1, 'not': 1, ',': 4, 'i': 4, 'am': 3, 'what': 1, 'think': 1, 'therefore': 1, 'da': 3, 'let': 2, 'it': 2}\n",
      "[['to', 'do', 'is', 'to', 'be', '.', 'to', 'be', 'is', 'to', 'do', '.'], ['to', 'be', 'or', 'not', 'to', 'be', ',', 'i', 'am', 'what', 'i', 'am', '.'], ['i', 'think', 'therefore', 'i', 'am', '.', 'do', 'be', 'do', 'be', 'do', '.'], ['do', 'do', 'do', ',', 'da', 'da', 'da', ',', 'let', 'it', 'be', ',', 'let', 'it', 'be']]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:00:01.870973Z",
     "start_time": "2025-04-04T06:00:01.860461Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "for doc in docs:\n",
    "    delim = re.compile(r'[\\s,.]+') #공백문자, ',', '.'으로 구분\n",
    "    tokens = delim.split(doc.lower()) #정규표현식으로 sent 텍스트 분할\n",
    "    if tokens[-1] == '' :   tokens = tokens[:-1] #맨 뒤에 빈 문자열 제거\n"
   ],
   "id": "e9dd84ff158032be",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:00:18.029846Z",
     "start_time": "2025-04-04T06:00:18.017308Z"
    }
   },
   "cell_type": "code",
   "source": "vocab['am']",
   "id": "e3e9d9f5569c6d9a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "eaa31db568977c89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:00:28.308065Z",
     "start_time": "2025-04-04T06:00:28.300066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "doc_tokens = []     # python list\n",
    "for doc in docs:\n",
    "    delim = re.compile(r'[\\s,.]+')\n",
    "    tokens = delim.split(doc.lower()) #정규표현식으로 sent 텍스트 분할\n",
    "    if tokens[-1] == '' :   tokens = tokens[:-1] \n",
    "    doc_tokens.append(tokens)\n",
    "\n",
    "vocab = Counter(sum(doc_tokens, []))\n",
    "print(type(vocab))\n",
    "vocab "
   ],
   "id": "8122f56052abb6dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Counter({'do': 8,\n",
       "         'be': 8,\n",
       "         'to': 6,\n",
       "         'i': 4,\n",
       "         'am': 3,\n",
       "         'da': 3,\n",
       "         'is': 2,\n",
       "         'let': 2,\n",
       "         'it': 2,\n",
       "         'or': 1,\n",
       "         'not': 1,\n",
       "         'what': 1,\n",
       "         'think': 1,\n",
       "         'therefore': 1})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:00:46.585009Z",
     "start_time": "2025-04-04T06:00:46.573500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "\n",
    "doc_tokens = []     # python list\n",
    "for doc in docs:\n",
    "    delim = re.compile(r'[\\s,.]+')\n",
    "    tokens = delim.split(doc.lower()) #정규표현식으로 sent 텍스트 분할\n",
    "    if tokens[-1] == '' :   tokens = tokens[:-1] \n",
    "    doc_tokens.append(tokens)\n",
    "\n",
    "vocab = FreqDist(np.hstack(doc_tokens))\n",
    "print(type(vocab))\n",
    "print(vocab['to'])\n",
    "vocab"
   ],
   "id": "4b8dcd1d3370294a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'nltk.probability.FreqDist'>\n",
      "6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FreqDist({'do': 8, 'be': 8, 'to': 6, 'i': 4, 'am': 3, 'da': 3, 'is': 2, 'let': 2, 'it': 2, 'or': 1, ...})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:00:55.176361Z",
     "start_time": "2025-04-04T06:00:55.167358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vocab = vocab.most_common()\n",
    "vocab"
   ],
   "id": "48a9ff8a7b75f7d4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('do', 8),\n",
       " ('be', 8),\n",
       " ('to', 6),\n",
       " ('i', 4),\n",
       " ('am', 3),\n",
       " ('da', 3),\n",
       " ('is', 2),\n",
       " ('let', 2),\n",
       " ('it', 2),\n",
       " ('or', 1),\n",
       " ('not', 1),\n",
       " ('what', 1),\n",
       " ('think', 1),\n",
       " ('therefore', 1)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:01:00.797605Z",
     "start_time": "2025-04-04T06:01:00.782357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_to_id = {word[0] : id for id, word in enumerate(vocab)}\n",
    "word_to_id"
   ],
   "id": "9adcf405f839d62",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'do': 0,\n",
       " 'be': 1,\n",
       " 'to': 2,\n",
       " 'i': 3,\n",
       " 'am': 4,\n",
       " 'da': 5,\n",
       " 'is': 6,\n",
       " 'let': 7,\n",
       " 'it': 8,\n",
       " 'or': 9,\n",
       " 'not': 10,\n",
       " 'what': 11,\n",
       " 'think': 12,\n",
       " 'therefore': 13}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:01:05.309603Z",
     "start_time": "2025-04-04T06:01:05.301603Z"
    }
   },
   "cell_type": "code",
   "source": "word_to_id = {word[0] : id+1 for id, word in enumerate(vocab)}",
   "id": "c446902fcb072a5",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:01:23.298271Z",
     "start_time": "2025-04-04T06:01:23.280762Z"
    }
   },
   "cell_type": "code",
   "source": [
    "id_to_word = {id : word[0] for id, word in enumerate(vocab)}\n",
    "id_to_word"
   ],
   "id": "d8409fe1df23677e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'do',\n",
       " 1: 'be',\n",
       " 2: 'to',\n",
       " 3: 'i',\n",
       " 4: 'am',\n",
       " 5: 'da',\n",
       " 6: 'is',\n",
       " 7: 'let',\n",
       " 8: 'it',\n",
       " 9: 'or',\n",
       " 10: 'not',\n",
       " 11: 'what',\n",
       " 12: 'think',\n",
       " 13: 'therefore'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:56:43.129309Z",
     "start_time": "2025-04-04T06:56:42.304023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def buildDict(docs):\n",
    "    doc_tokens = []     # python list\n",
    "    for doc in docs:\n",
    "        delim = re.compile(r'[\\s,.]+')\n",
    "        tokens = delim.split(doc.lower()) \n",
    "        if tokens[-1] == '' :   tokens = tokens[:-1] \n",
    "        doc_tokens.append(tokens)\n",
    "\n",
    "    vocab = FreqDist(np.hstack(doc_tokens))\n",
    "    vocab = vocab.most_common()\n",
    "    word_to_id = {word[0] : id for id, word in enumerate(vocab)}\n",
    "    id_to_word = {id : word[0] for id, word in enumerate(vocab)}\n",
    "    return doc_tokens, vocab, word_to_id, id_to_word    \n",
    "\n",
    "docs = []\n",
    "docs.append('To do is to be. To be is to do.')\n",
    "docs.append('To be or not to be. I am what I am.')\n",
    "docs.append('I think therefore I am. Do be do be do.')\n",
    "docs.append('Do do do, da da da, Let it be, let it be.')\n",
    "\n",
    "doc_tokens, vocab, word_to_id, id_to_word = buildDict(docs)\n",
    "\n",
    "print(doc_tokens)\n",
    "print(vocab)\n",
    "print(word_to_id)\n",
    "print(id_to_word)"
   ],
   "id": "4551e0bf8a1c19cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['to', 'do', 'is', 'to', 'be', 'to', 'be', 'is', 'to', 'do'], ['to', 'be', 'or', 'not', 'to', 'be', 'i', 'am', 'what', 'i', 'am'], ['i', 'think', 'therefore', 'i', 'am', 'do', 'be', 'do', 'be', 'do'], ['do', 'do', 'do', 'da', 'da', 'da', 'let', 'it', 'be', 'let', 'it', 'be']]\n",
      "[('do', 8), ('be', 8), ('to', 6), ('i', 4), ('am', 3), ('da', 3), ('is', 2), ('let', 2), ('it', 2), ('or', 1), ('not', 1), ('what', 1), ('think', 1), ('therefore', 1)]\n",
      "{'do': 0, 'be': 1, 'to': 2, 'i': 3, 'am': 4, 'da': 5, 'is': 6, 'let': 7, 'it': 8, 'or': 9, 'not': 10, 'what': 11, 'think': 12, 'therefore': 13}\n",
      "{0: 'do', 1: 'be', 2: 'to', 3: 'i', 4: 'am', 5: 'da', 6: 'is', 7: 'let', 8: 'it', 9: 'or', 10: 'not', 11: 'what', 12: 'think', 13: 'therefore'}\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:56:43.710839Z",
     "start_time": "2025-04-04T06:56:43.695821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sent = 'Thomas Jefferson began building Monticello at the age of 26.'\n",
    "token = sent.split() #간단한 단어 분리\n",
    "print(token)"
   ],
   "id": "1949e632b2a18da0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thomas', 'Jefferson', 'began', 'building', 'Monticello', 'at', 'the', 'age', 'of', '26.']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:56:44.305351Z",
     "start_time": "2025-04-04T06:56:44.286835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "one_hot_vectors = [] #토큰별 원-핫 벡터의 리스트\n",
    "\n",
    "for idx, word in enumerate(token): #토큰마다 원-핫 벡터 생성 \n",
    "    vector = [0 for _ in token] #모든 값이 0인 사전크기 리스트 생성\n",
    "    vector[token.index(word)] = 1 #사전에서 단어가 출현한 위치만 1로 설정 \n",
    "    one_hot_vectors.append(vector) #만들어진 벡터를 원-핫 벡터에 저장 \n",
    "\n",
    "#결과출력\n",
    "for idx, vec in enumerate(one_hot_vectors):\n",
    "    print('token = {} : vector : {}'.format(token[idx], vec))"
   ],
   "id": "fdb1203bb494ba9b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token = Thomas : vector : [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "token = Jefferson : vector : [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "token = began : vector : [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "token = building : vector : [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "token = Monticello : vector : [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "token = at : vector : [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "token = the : vector : [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "token = age : vector : [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "token = of : vector : [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "token = 26. : vector : [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T06:56:45.864170Z",
     "start_time": "2025-04-04T06:56:44.856801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(one_hot_vectors, columns=vocab)\n",
    "print(df)"
   ],
   "id": "a690f50a827c1d9e",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "14 columns passed, passed data had 10 columns",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "File \u001B[1;32m~\\anaconda3\\envs\\ocr\\lib\\site-packages\\pandas\\core\\internals\\construction.py:939\u001B[0m, in \u001B[0;36m_finalize_columns_and_data\u001B[1;34m(content, columns, dtype)\u001B[0m\n\u001B[0;32m    938\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 939\u001B[0m     columns \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_or_indexify_columns\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcontents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001B[39;00m\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ocr\\lib\\site-packages\\pandas\\core\\internals\\construction.py:986\u001B[0m, in \u001B[0;36m_validate_or_indexify_columns\u001B[1;34m(content, columns)\u001B[0m\n\u001B[0;32m    984\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_mi_list \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(columns) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mlen\u001B[39m(content):  \u001B[38;5;66;03m# pragma: no cover\u001B[39;00m\n\u001B[0;32m    985\u001B[0m     \u001B[38;5;66;03m# caller's responsibility to check for this...\u001B[39;00m\n\u001B[1;32m--> 986\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[0;32m    987\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(columns)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m columns passed, passed data had \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    988\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(content)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m columns\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    989\u001B[0m     )\n\u001B[0;32m    990\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_mi_list:\n\u001B[0;32m    991\u001B[0m     \u001B[38;5;66;03m# check if nested list column, length of each sub-list should be equal\u001B[39;00m\n",
      "\u001B[1;31mAssertionError\u001B[0m: 14 columns passed, passed data had 10 columns",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m df \u001B[38;5;241m=\u001B[39m \u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataFrame\u001B[49m\u001B[43m(\u001B[49m\u001B[43mone_hot_vectors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(df)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ocr\\lib\\site-packages\\pandas\\core\\frame.py:851\u001B[0m, in \u001B[0;36mDataFrame.__init__\u001B[1;34m(self, data, index, columns, dtype, copy)\u001B[0m\n\u001B[0;32m    849\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    850\u001B[0m         columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[1;32m--> 851\u001B[0m     arrays, columns, index \u001B[38;5;241m=\u001B[39m \u001B[43mnested_data_to_arrays\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    852\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001B[39;49;00m\n\u001B[0;32m    853\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001B[39;49;00m\n\u001B[0;32m    854\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    855\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    856\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# type: ignore[arg-type]\u001B[39;49;00m\n\u001B[0;32m    857\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    859\u001B[0m     mgr \u001B[38;5;241m=\u001B[39m arrays_to_mgr(\n\u001B[0;32m    860\u001B[0m         arrays,\n\u001B[0;32m    861\u001B[0m         columns,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    864\u001B[0m         typ\u001B[38;5;241m=\u001B[39mmanager,\n\u001B[0;32m    865\u001B[0m     )\n\u001B[0;32m    866\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ocr\\lib\\site-packages\\pandas\\core\\internals\\construction.py:520\u001B[0m, in \u001B[0;36mnested_data_to_arrays\u001B[1;34m(data, columns, index, dtype)\u001B[0m\n\u001B[0;32m    517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_named_tuple(data[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;129;01mand\u001B[39;00m columns \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    518\u001B[0m     columns \u001B[38;5;241m=\u001B[39m ensure_index(data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m_fields)\n\u001B[1;32m--> 520\u001B[0m arrays, columns \u001B[38;5;241m=\u001B[39m \u001B[43mto_arrays\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    521\u001B[0m columns \u001B[38;5;241m=\u001B[39m ensure_index(columns)\n\u001B[0;32m    523\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m index \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ocr\\lib\\site-packages\\pandas\\core\\internals\\construction.py:845\u001B[0m, in \u001B[0;36mto_arrays\u001B[1;34m(data, columns, dtype)\u001B[0m\n\u001B[0;32m    842\u001B[0m     data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mtuple\u001B[39m(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m data]\n\u001B[0;32m    843\u001B[0m     arr \u001B[38;5;241m=\u001B[39m _list_to_arrays(data)\n\u001B[1;32m--> 845\u001B[0m content, columns \u001B[38;5;241m=\u001B[39m \u001B[43m_finalize_columns_and_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    846\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m content, columns\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\ocr\\lib\\site-packages\\pandas\\core\\internals\\construction.py:942\u001B[0m, in \u001B[0;36m_finalize_columns_and_data\u001B[1;34m(content, columns, dtype)\u001B[0m\n\u001B[0;32m    939\u001B[0m     columns \u001B[38;5;241m=\u001B[39m _validate_or_indexify_columns(contents, columns)\n\u001B[0;32m    940\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[0;32m    941\u001B[0m     \u001B[38;5;66;03m# GH#26429 do not raise user-facing AssertionError\u001B[39;00m\n\u001B[1;32m--> 942\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(err) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merr\u001B[39;00m\n\u001B[0;32m    944\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(contents) \u001B[38;5;129;01mand\u001B[39;00m contents[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mdtype \u001B[38;5;241m==\u001B[39m np\u001B[38;5;241m.\u001B[39mobject_:\n\u001B[0;32m    945\u001B[0m     contents \u001B[38;5;241m=\u001B[39m convert_object_array(contents, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "\u001B[1;31mValueError\u001B[0m: 14 columns passed, passed data had 10 columns"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "c93ff7718079a605"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
