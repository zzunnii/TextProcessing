{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-02T05:55:12.074601Z",
     "start_time": "2025-06-02T05:55:11.967081Z"
    }
   },
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw = stopwords.words('english')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tjdwn\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T06:02:12.685072Z",
     "start_time": "2025-06-02T06:02:12.676068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def buildDict(docs):          # python list\n",
    "    doc_tokens = []\n",
    "    for doc in docs:\n",
    "        delim = re.compile(r'[\\s,.]+')\n",
    "        tokens = delim.split(doc.lower())\n",
    "        tokens = [t for t in tokens if t not in sw]\n",
    "        if tokens[-1] == '.': tokens = tokens[:-1]\n",
    "        doc_tokens.append(tokens)\n",
    "\n",
    "    vocab = FreqDist(np.hstack(doc_tokens))\n",
    "    vocab = vocab.most_common()\n",
    "    word_to_id = {word[0]: id for id, word in enumerate(vocab)}\n",
    "    id_to_word = {id: word[0] for id, word in enumerate(vocab)}\n",
    "    corpus = np.array([word_to_id[word[0]] for word in vocab])\n",
    "    return doc_tokens, corpus, word_to_id, id_to_word"
   ],
   "id": "38c6780149af242a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T06:02:13.122619Z",
     "start_time": "2025-06-02T06:02:13.115620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "with open('./bts_korean.txt', 'r', encoding='utf-8') as f:\n",
    "    kor_docs = f.readlines()\n",
    "\n",
    "with open('./bts.txt', 'r', encoding='utf-8') as f:\n",
    "    docs = f.readlines()\n",
    "\n",
    "for id, doc in enumerate(docs):\n",
    "    print('[{}] : {}...'.format(id, doc[:30]))\n",
    "\n",
    "doc_tokens, corpus, word_to_id, id_to_word = buildDict(docs)\n"
   ],
   "id": "53fb3853665e9b2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] : BTS, also known as the Bangtan...\n",
      "[1] : [5] The septet—consisting of m...\n",
      "[2] : Originally a hip hop group, th...\n",
      "[3] : Their lyrics, often focused on...\n",
      "[4] : Their work also often referenc...\n",
      "[5] : After debuting in 2013 with th...\n",
      "[6] : The group's second Korean stud...\n",
      "[7] : By 2017, BTS crossed into the ...\n",
      "[8] : They became the first Korean g...\n",
      "[9] : BTS became one of the few grou...\n",
      "[10] : In 2020, BTS became the first ...\n",
      "[11] : Their follow-up releases \"Sava...\n",
      "[12] : Having sold over 20 million al...\n",
      "[13] : They are the first Asian and n...\n",
      "[14] : Featured on Time's internation...\n",
      "[15] : The group's numerous accolades...\n",
      "[16] : Outside of music, they partner...\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T06:02:16.512867Z",
     "start_time": "2025-06-02T06:02:16.502869Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    '''\n",
    "    동시발생 행렬 생성\n",
    "    :param corpus: 말뭉치(단어 ID 목록)\n",
    "    :param vocab_size: 어휘 수\n",
    "    :param window_size: 윈도우 크기(윈도우 크기가 1이면 타깃 단어 좌우 한 단어씩이 맥락에 포함)\n",
    "    :return: 동시발생 행렬\n",
    "    '''\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix\n"
   ],
   "id": "2f513fd394612b33",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T06:02:16.783516Z",
     "start_time": "2025-06-02T06:02:16.771514Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def ppmi(C, verbose=False, eps=1e-8):\n",
    "    '''\n",
    "    PPMI (점별 상호정보량) 생성\n",
    "    :param C: 동시발생 행렬\n",
    "    :param verbose: 진행 상황을 출력할지 여부\n",
    "    :return:\n",
    "    '''\n",
    "    M = np.zeros_like(C, dtype=np.float32)\n",
    "    N = np.sum(C)\n",
    "    S = np.sum(C, axis=0)\n",
    "    total = C.shape[0] * C.shape[1]\n",
    "    cnt = 0\n",
    "\n",
    "    for i in range(C.shape[0]):\n",
    "        for j in range(C.shape[1]):\n",
    "            pmi = np.log2(C[i, j] * N / (S[j] * S[i]) + eps)\n",
    "            M[i, j] = max(0, pmi)\n",
    "\n",
    "            if verbose:\n",
    "                cnt += 1\n",
    "                if cnt % (total // 100 + 1) == 0:\n",
    "                    print('%.1f%% 완료' % (100 * cnt / total))\n",
    "\n",
    "    return M\n"
   ],
   "id": "54fa32e71a23392c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T06:02:17.085038Z",
     "start_time": "2025-06-02T06:02:17.016038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "window_size = 2\n",
    "\n",
    "vocab_size = len(word_to_id)\n",
    "print('동시발생행렬 계산')\n",
    "C = create_co_matrix(corpus, vocab_size, window_size)\n",
    "W = ppmi(C)\n",
    "\n",
    "print(C[0, :10])\n",
    "print(W[0, :10])\n"
   ],
   "id": "f14e07f8bbea4f1a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "동시발생행렬 계산\n",
      "[0 1 1 0 0 0 0 0 0 0]\n",
      "[0.        7.4178524 7.0028152 0.        0.        0.        0.\n",
      " 0.        0.        0.       ]\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T06:02:17.613080Z",
     "start_time": "2025-06-02T06:02:17.555577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "wordvec_size = 100\n",
    "U, S, V = randomized_svd(W, n_components=wordvec_size, n_iter=5, random_state=None)\n"
   ],
   "id": "421650a7a42ec5ac",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T06:02:18.626158Z",
     "start_time": "2025-06-02T06:02:18.610158Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "    if query not in word_to_id:\n",
    "        print('{}를 찾을 수 없음.'.format(query))\n",
    "        return\n",
    "\n",
    "    word_vector = np.array([word_matrix[word_to_id[query]]])  # 쿼리단어 벡터 추출\n",
    "    word_vector = word_vector.reshape(1, -1)  # cosine_similarity 위해 벡터 형상 조정\n",
    "    sim = cosine_similarity(word_vector, word_matrix)\n",
    "    sim = sim[0]  # 벡터 형상 조정 ([[]] -> [])\n",
    "\n",
    "    sim = [(id, cos) for id, cos in enumerate(sim)]  # id, 유사도 쌍으로 정리\n",
    "    sim = sorted(sim, key=lambda x: x[1], reverse=True)  # 유사도 높은 순 정렬\n",
    "\n",
    "    return sim[1:top+1]  # 자기 자신 제외하고 top개 반환\n"
   ],
   "id": "69903919d0f3316a",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T06:02:19.199055Z",
     "start_time": "2025-06-02T06:02:19.191696Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rank = most_similar('world', word_to_id, id_to_word, U)\n",
    "for r in rank:\n",
    "    print(id_to_word[r[0]], r[1])\n"
   ],
   "id": "4bbd3ef67224ced7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artist 0.52162355\n",
      "known 0.5026817\n",
      "influential 0.41057464\n",
      "stadium 0.3954757\n",
      "best-selling 0.22336486\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d055c5b49b2c9688"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
