{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-13T05:44:42.026848Z",
     "start_time": "2025-04-13T05:44:41.032742Z"
    }
   },
   "source": [
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:54:36.241893Z",
     "start_time": "2025-04-13T09:54:36.234384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def buildDict(docs):\n",
    "    doc_tokens = []\n",
    "    for doc in docs:\n",
    "        delim = re.compile(r'[\\s,.]+')\n",
    "        tokens = delim.split(doc.lower())\n",
    "        if tokens[-1] == '' : tokens = tokens[:-1]\n",
    "        doc_tokens.append(tokens)\n",
    "        \n",
    "    vocab = FreqDist(np.hstack(doc_tokens))\n",
    "    vocab = vocab.most_common()\n",
    "    word_to_id = {word[0] : id for id, word in enumerate(vocab)}\n",
    "    id_to_word = {id:word[0] for id, word in enumerate(vocab)}\n",
    "    \n",
    "    return doc_tokens, vocab, word_to_id, id_to_word"
   ],
   "id": "2a77f1b35ea1aec8",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:54:37.090514Z",
     "start_time": "2025-04-13T09:54:37.081515Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = []\n",
    "docs.append('To do is to be. To be is to do.')\n",
    "docs.append('To be or not to be, I am what I am.')\n",
    "docs.append('I think therefore I am. Do be do be do.')\n",
    "docs.append('Do do do, da da da, Let it be, let it be')\n",
    "\n",
    "doc_tokens, vocab, word_to_id, id_to_word = buildDict(docs)"
   ],
   "id": "70e90e37c6689d1c",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T05:52:00.239352Z",
     "start_time": "2025-04-13T05:52:00.230352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "import math\n"
   ],
   "id": "c107c3f8033acd18",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T05:52:00.506889Z",
     "start_time": "2025-04-13T05:52:00.502888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_vectors = []\n",
    "for doc in doc_tokens:\n",
    "    vec = [0.0 for _ in range((len(word_to_id)))]\n",
    "    word_count = Counter(doc)\n",
    "    for key, value in word_count.items():\n",
    "        vec[word_to_id[key]] = 1+math.log2(value)\n",
    "    tf_vectors.append(vec)\n",
    "    "
   ],
   "id": "a229f956ba642419",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T05:52:26.315364Z",
     "start_time": "2025-04-13T05:52:26.292844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tf_vectors, columns=id_to_word.values())\n",
    "df"
   ],
   "id": "f94a4f5accf52985",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "         do   be   to    i   am        da   is  let   it   or  not  what  \\\n",
       "0  2.000000  2.0  3.0  0.0  0.0  0.000000  2.0  0.0  0.0  0.0  0.0   0.0   \n",
       "1  0.000000  2.0  2.0  2.0  2.0  0.000000  0.0  0.0  0.0  1.0  1.0   1.0   \n",
       "2  2.584963  2.0  0.0  2.0  1.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   \n",
       "3  2.584963  2.0  0.0  0.0  0.0  2.584963  0.0  2.0  2.0  0.0  0.0   0.0   \n",
       "\n",
       "   think  therefore  \n",
       "0    0.0        0.0  \n",
       "1    0.0        0.0  \n",
       "2    1.0        1.0  \n",
       "3    0.0        0.0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>do</th>\n",
       "      <th>be</th>\n",
       "      <th>to</th>\n",
       "      <th>i</th>\n",
       "      <th>am</th>\n",
       "      <th>da</th>\n",
       "      <th>is</th>\n",
       "      <th>let</th>\n",
       "      <th>it</th>\n",
       "      <th>or</th>\n",
       "      <th>not</th>\n",
       "      <th>what</th>\n",
       "      <th>think</th>\n",
       "      <th>therefore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.584963</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.584963</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.584963</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T05:59:53.227698Z",
     "start_time": "2025-04-13T05:59:53.217190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idf = { }\n",
    "for id, _ in id_to_word.items():\n",
    "    idf[id] = 0.0\n",
    "    for doc in tf_vectors:\n",
    "        if doc[id] > 0:\n",
    "            idf[id] += 1"
   ],
   "id": "b5d426189374e9eb",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T06:00:49.948013Z",
     "start_time": "2025-04-13T06:00:49.939508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = len(tf_vectors)\n",
    "idf = {id : math.log2(N/val) for id, val in idf.items()}"
   ],
   "id": "a69f2cca48ae7bf8",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T06:01:40.553841Z",
     "start_time": "2025-04-13T06:01:40.544331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.Series(idf.values(), index = idf.keys())\n",
    "print(df)"
   ],
   "id": "c74d8c97d7977a38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     0.415037\n",
      "1     0.000000\n",
      "2     1.000000\n",
      "3     1.000000\n",
      "4     1.000000\n",
      "5     2.000000\n",
      "6     2.000000\n",
      "7     2.000000\n",
      "8     2.000000\n",
      "9     2.000000\n",
      "10    2.000000\n",
      "11    2.000000\n",
      "12    2.000000\n",
      "13    2.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T06:02:31.034573Z",
     "start_time": "2025-04-13T06:02:31.025574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "idf_list = [val for _, val in idf.items()]\n",
    "tfidf = np.array([np.multiply(tf, idf_list) for tf in tf_vectors])"
   ],
   "id": "9ca414591de6d102",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T06:02:48.428629Z",
     "start_time": "2025-04-13T06:02:48.407122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.DataFrame(tfidf, columns=id_to_word.values())\n",
    "print(df)\n",
    "print(df.T)"
   ],
   "id": "7a304778d824297d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         do   be   to    i   am        da   is  let   it   or  not  what  \\\n",
      "0  0.830075  0.0  3.0  0.0  0.0  0.000000  4.0  0.0  0.0  0.0  0.0   0.0   \n",
      "1  0.000000  0.0  2.0  2.0  2.0  0.000000  0.0  0.0  0.0  2.0  2.0   2.0   \n",
      "2  1.072856  0.0  0.0  2.0  1.0  0.000000  0.0  0.0  0.0  0.0  0.0   0.0   \n",
      "3  1.072856  0.0  0.0  0.0  0.0  5.169925  0.0  4.0  4.0  0.0  0.0   0.0   \n",
      "\n",
      "   think  therefore  \n",
      "0    0.0        0.0  \n",
      "1    0.0        0.0  \n",
      "2    2.0        2.0  \n",
      "3    0.0        0.0  \n",
      "                  0    1         2         3\n",
      "do         0.830075  0.0  1.072856  1.072856\n",
      "be         0.000000  0.0  0.000000  0.000000\n",
      "to         3.000000  2.0  0.000000  0.000000\n",
      "i          0.000000  2.0  2.000000  0.000000\n",
      "am         0.000000  2.0  1.000000  0.000000\n",
      "da         0.000000  0.0  0.000000  5.169925\n",
      "is         4.000000  0.0  0.000000  0.000000\n",
      "let        0.000000  0.0  0.000000  4.000000\n",
      "it         0.000000  0.0  0.000000  4.000000\n",
      "or         0.000000  2.0  0.000000  0.000000\n",
      "not        0.000000  2.0  0.000000  0.000000\n",
      "what       0.000000  2.0  0.000000  0.000000\n",
      "think      0.000000  0.0  2.000000  0.000000\n",
      "therefore  0.000000  0.0  2.000000  0.000000\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:54:41.233606Z",
     "start_time": "2025-04-13T09:54:41.227605Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "labels = encoder.fit_transform([word for word, id in vocab])\n",
    "for label in labels:\n",
    "    print('[{:2d} : {}]'.format(label, encoder.classes_[label]))\n",
    "print(encoder.classes_)"
   ],
   "id": "65c3096b79a690f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3 : do]\n",
      "[ 1 : be]\n",
      "[12 : to]\n",
      "[ 4 : i]\n",
      "[ 0 : am]\n",
      "[ 2 : da]\n",
      "[ 5 : is]\n",
      "[ 7 : let]\n",
      "[ 6 : it]\n",
      "[ 9 : or]\n",
      "[ 8 : not]\n",
      "[13 : what]\n",
      "[11 : think]\n",
      "[10 : therefore]\n",
      "['am' 'be' 'da' 'do' 'i' 'is' 'it' 'let' 'not' 'or' 'therefore' 'think'\n",
      " 'to' 'what']\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:55:30.302427Z",
     "start_time": "2025-04-13T09:55:30.294426Z"
    }
   },
   "cell_type": "code",
   "source": [
    "encode_data = [encoder.transform(doc_token) for doc_token in doc_tokens]\n",
    "print(encode_data)\n",
    "for code in encode_data:\n",
    "    print(encoder.inverse_transform(code))"
   ],
   "id": "f6b5003f8ea505b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([12,  3,  5, 12,  1, 12,  1,  5, 12,  3]), array([12,  1,  9,  8, 12,  1,  4,  0, 13,  4,  0]), array([ 4, 11, 10,  4,  0,  3,  1,  3,  1,  3]), array([3, 3, 3, 2, 2, 2, 7, 6, 1, 7, 6, 1])]\n",
      "['to' 'do' 'is' 'to' 'be' 'to' 'be' 'is' 'to' 'do']\n",
      "['to' 'be' 'or' 'not' 'to' 'be' 'i' 'am' 'what' 'i' 'am']\n",
      "['i' 'think' 'therefore' 'i' 'am' 'do' 'be' 'do' 'be' 'do']\n",
      "['do' 'do' 'do' 'da' 'da' 'da' 'let' 'it' 'be' 'let' 'it' 'be']\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:56:07.293304Z",
     "start_time": "2025-04-13T09:56:07.286303Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "oh_encoder = OneHotEncoder(categories='auto')\n",
    "labels = labels.reshape(-1, 1)\n",
    "oh_labels = oh_encoder.fit_transform(labels)"
   ],
   "id": "e888c28722fb6270",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:56:10.878519Z",
     "start_time": "2025-04-13T09:56:10.864520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "oh_vectors = []\n",
    "for data in encode_data:\n",
    "    data = data.reshape(-1,1)\n",
    "    oh_vector = oh_encoder.transform(data).toarray()\n",
    "    oh_vectors.append(oh_vector)"
   ],
   "id": "cd870d71d0c58e84",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:56:18.558232Z",
     "start_time": "2025-04-13T09:56:18.538718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for data, oh_vector in zip(encode_data, oh_vectors):\n",
    "    print(encoder.inverse_transform(data))\n",
    "    print(oh_vector)"
   ],
   "id": "bcacb456349cbb7c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['to' 'do' 'is' 'to' 'be' 'to' 'be' 'is' 'to' 'do']\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "['to' 'be' 'or' 'not' 'to' 'be' 'i' 'am' 'what' 'i' 'am']\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "['i' 'think' 'therefore' 'i' 'am' 'do' 'be' 'do' 'be' 'do']\n",
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "['do' 'do' 'do' 'da' 'da' 'da' 'let' 'it' 'be' 'let' 'it' 'be']\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:57:02.016454Z",
     "start_time": "2025-04-13T09:57:02.008455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "docs = []\n",
    "docs.append('To do is to be. To be is to do.')\n",
    "docs.append('To be or not to be. II am what II am')\n",
    "docs.append('II think therefore II am. Do be do be do.')\n",
    "docs.append('Do do do da da da. Let it be let it be.')\n",
    "\n",
    "cnt_vectr = CountVectorizer()\n",
    "vectors = cnt_vectr.fit_transform(docs)\n",
    "\n",
    "print(cnt_vectr.vocabulary_)\n",
    "print(cnt_vectr.get_feature_names_out())\n",
    "print(vectors.toarray())\n",
    "print(pd.DataFrame(vectors.toarray(),\n",
    "                   columns=cnt_vectr.get_feature_names_out()))"
   ],
   "id": "24953af622c145ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 12, 'do': 3, 'is': 5, 'be': 1, 'or': 9, 'not': 8, 'ii': 4, 'am': 0, 'what': 13, 'think': 11, 'therefore': 10, 'da': 2, 'let': 7, 'it': 6}\n",
      "['am' 'be' 'da' 'do' 'ii' 'is' 'it' 'let' 'not' 'or' 'therefore' 'think'\n",
      " 'to' 'what']\n",
      "[[0 2 0 2 0 2 0 0 0 0 0 0 4 0]\n",
      " [2 2 0 0 2 0 0 0 1 1 0 0 2 1]\n",
      " [1 2 0 3 2 0 0 0 0 0 1 1 0 0]\n",
      " [0 2 3 3 0 0 2 2 0 0 0 0 0 0]]\n",
      "   am  be  da  do  ii  is  it  let  not  or  therefore  think  to  what\n",
      "0   0   2   0   2   0   2   0    0    0   0          0      0   4     0\n",
      "1   2   2   0   0   2   0   0    0    1   1          0      0   2     1\n",
      "2   1   2   0   3   2   0   0    0    0   0          1      1   0     0\n",
      "3   0   2   3   3   0   0   2    2    0   0          0      0   0     0\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T09:57:19.659339Z",
     "start_time": "2025-04-13T09:57:19.652335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfidf = TfidfVectorizer().fit(docs)\n",
    "dtm = tfidf.transform(docs).toarray()\n",
    "\n",
    "df = pd.DataFrame(dtm, columns=tfidf.get_feature_names_out())\n",
    "print(df)\n",
    "print(sorted(tfidf.vocabulary_.items()))"
   ],
   "id": "1735a00870fadc63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         am        be        da        do        ii        is        it  \\\n",
      "0  0.000000  0.255666  0.000000  0.312717  0.000000  0.489931  0.000000   \n",
      "1  0.464005  0.307120  0.000000  0.000000  0.464005  0.000000  0.000000   \n",
      "2  0.251031  0.332310  0.000000  0.609695  0.502063  0.000000  0.000000   \n",
      "3  0.000000  0.223758  0.643179  0.410533  0.000000  0.000000  0.428786   \n",
      "\n",
      "        let       not        or  therefore     think        to      what  \n",
      "0  0.000000  0.000000  0.000000   0.000000  0.000000  0.772535  0.000000  \n",
      "1  0.000000  0.294266  0.294266   0.000000  0.000000  0.464005  0.294266  \n",
      "2  0.000000  0.000000  0.000000   0.318401  0.318401  0.000000  0.000000  \n",
      "3  0.428786  0.000000  0.000000   0.000000  0.000000  0.000000  0.000000  \n",
      "[('am', 0), ('be', 1), ('da', 2), ('do', 3), ('ii', 4), ('is', 5), ('it', 6), ('let', 7), ('not', 8), ('or', 9), ('therefore', 10), ('think', 11), ('to', 12), ('what', 13)]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a5e1897f24cb93c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
